{
    "$schema": "http://json-schema.org/schema#",
    "type": "object",
    "properties": {
        "huggingface": {
            "type": "object",
            "properties": {
                "model": {
                    "type": "string",
                    "title": "Model",
                    "description": "The HuggingFace model to deploy (Hint: For a simple, lightweight demo try ise-uiuc/Magicoder-S-DS-6.7B)"
                },
                "token": {
                    "type": "string",
                    "title": "Access Token",
                    "description": "The HuggingFace access token to use for installing gated models.",
                    "default": ""
                }
            },
            "required": ["model"]
        },
        "ui": {
            "type": "object",
            "properties": {
                "appSettings": {
                    "type": "object",
                    "properties": {
                        "model_name": {
                            "type": "string",
                            "title": "Model Name",
                            "description": "Model name supplied to OpenAI client in frontend web app. Should match huggingface.model above."
                        },
                        "model_instruction": {
                            "type": "string",
                            "title": "Model instruction",
                            "description": "The initial model prompt (i.e. the hidden instructions) to use when generating responses."
                        },
                        "llm_max_tokens": {
                            "type": "number",
                            "title": "LLM temperature",
                            "description": "The maximum number of new [tokens](https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_tokens) to generate for each LLM responses."
                        },
                        "llm_temperature": {
                            "type": "number",
                            "title": "LLM temperature",
                            "description": "The '[temperature](https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature)' value to use when generating LLM responses."
                        },
                        "llm_top_p": {
                            "type": "number",
                            "title": "LLM Top P",
                            "description": "The [top p](https://platform.openai.com/docs/api-reference/chat/create#chat-create-top_p) value to use when generating LLM responses."
                        },
                        "llm_presence_penalty": {
                            "type": "number",
                            "title": "LLM Presence Penalty",
                            "description": "The [presence penalty](https://platform.openai.com/docs/api-reference/chat/create#chat-create-presence_penalty) to use when generating LLM responses."
                        },
                        "llm_frequency_penalty": {
                            "type": "number",
                            "title": "LLM Frequency Penalty",
                            "description": "The [frequency_penalty](https://platform.openai.com/docs/api-reference/chat/create#chat-create-frequency_penalty) to use when generating LLM responses."
                        }

                    },
                    "required": ["model_name"]
                }
            }
        }
    },
    "required": ["huggingface"]
}