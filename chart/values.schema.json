{
    "$schema": "http://json-schema.org/schema#",
    "type": "object",
    "properties": {
        "huggingface": {
            "type": "object",
            "properties": {
                "model": {
                    "type": "string",
                    "title": "Model",
                    "description": "The [HuggingFace model](https://huggingface.co/models) to deploy (see [here](https://github.com/stackhpc/azimuth-llm?tab=readme-ov-file#tested-models) for a list of tested models).",
                    "default": "ise-uiuc/Magicoder-S-DS-6.7B"
                },
                "token": {
                    "type": ["string", "null"],
                    "title": "Access Token",
                    "description": "A HuggingFace [access token](https://huggingface.co/docs/hub/security-tokens). Required for [gated models](https://huggingface.co/docs/hub/en/models-gated) (e.g. Llama 3)."
                }
            },
            "required": ["model"]
        },
        "ui": {
            "type": "object",
            "properties": {
                "appSettings": {
                    "type": "object",
                    "properties": {
                        "hf_model_name": {
                            "type": "string",
                            "title": "Model Name",
                            "description": "Model name supplied to the OpenAI client in frontend web app. Should match huggingface.model above."
                        },
                        "hf_model_instruction": {
                            "type": "string",
                            "title": "Instruction",
                            "description": "The initial system prompt (i.e. the hidden instruction) to use when generating responses.",
                            "default": "You are a helpful AI assistant. Please respond appropriately."
                        },
                        "page_title": {
                            "type": "string",
                            "title": "Page Title",
                            "description": "The title to display at the top of the chat interface.",
                            "default": "Large Language Model"
                        },
                        "llm_max_tokens": {
                            "type": "integer",
                            "title": "Max Tokens",
                            "description": "The maximum number of new [tokens](https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_tokens) to generate for each LLM responses.",
                            "default": 1000
                        },
                        "llm_temperature": {
                            "type": "number",
                            "title": "LLM Temperature",
                            "description": "The [temperature](https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature) value to use when generating LLM responses.",
                            "default": 1,
                            "minimum": 0,
                            "maximum": 2
                        },
                        "llm_top_p": {
                            "type": "number",
                            "title": "LLM Top P",
                            "description": "The [top p](https://platform.openai.com/docs/api-reference/chat/create#chat-create-top_p) value to use when generating LLM responses.",
                            "default": 1,
                            "exclusiveMinimum": 0,
                            "maximum": 1
                        },
                        "llm_top_k": {
                            "type": "integer",
                            "title": "LLM Top K",
                            "description": "The [top k](https://docs.vllm.ai/en/stable/dev/sampling_params.html) value to use when generating LLM responses (must be an integer).",
                            "default": -1,
                            "minimum": -1
                        },
                        "llm_presence_penalty": {
                            "type": "number",
                            "title": "LLM Presence Penalty",
                            "description": "The [presence penalty](https://platform.openai.com/docs/api-reference/chat/create#chat-create-presence_penalty) to use when generating LLM responses.",
                            "default": 0,
                            "minimum": -2,
                            "maximum": 2
                        },
                        "llm_frequency_penalty": {
                            "type": "number",
                            "title": "LLM Frequency Penalty",
                            "description": "The [frequency_penalty](https://platform.openai.com/docs/api-reference/chat/create#chat-create-frequency_penalty) to use when generating LLM responses.",
                            "default": 0,
                            "minimum": -2,
                            "maximum": 2
                        }

                    },
                    "required": ["hf_model_name", "hf_model_instruction"]
                }
            }
        },
        "api": {
            "properties": {
                "modelMaxContextLength": {
                    "type": "integer",
                    "title": "Model Context Length",
                    "description": "An override for the maximum context length to use if the model's default is not suitable."
                },
                "image": {
                    "properties": {
                        "version": {
                            "type": "string",
                            "title": "vLLM version override",
                            "description": "The vLLM version to use as a backend. Must be a version tag from [this list](https://github.com/vllm-project/vllm/tags)",
                            "default": "v0.4.3"
                        }
                    }
                }
            }
        }
    }
}
