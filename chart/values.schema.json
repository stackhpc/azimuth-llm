{
    "$schema": "http://json-schema.org/schema#",
    "type": "object",
    "properties": {
        "huggingface": {
            "type": "object",
            "properties": {
                "model": {
                    "type": "string",
                    "title": "Model",
                    "description": "The [HuggingFace model](https://huggingface.co/models) to deploy (see [here](https://github.com/stackhpc/azimuth-llm?tab=readme-ov-file#tested-models) for a list of tested models).",
                    "default": "mistralai/Mistral-7B-Instruct-v0.2"
                },
                "token": {
                    "type": ["string", "null"],
                    "title": "Access Token",
                    "description": "A HuggingFace [access token](https://huggingface.co/docs/hub/security-tokens). Only required for [gated models](https://huggingface.co/docs/hub/en/models-gated) (e.g. Llama 2)."
                }
            },
            "required": ["model"]
        },
        "ui": {
            "type": "object",
            "properties": {
                "appSettings": {
                    "type": "object",
                    "properties": {
                        "hf_model_name": {
                            "type": "string",
                            "title": "Model Name",
                            "description": "Model name supplied to the OpenAI client in frontend web app. Should match huggingface.model above."
                        },
                        "hf_model_instruction": {
                            "type": "string",
                            "title": "Instruction",
                            "description": "The initial model prompt (i.e. the hidden instructions) to use when generating responses.",
                            "default": "You are a helpful AI assistant. Please respond appropriately."
                        },
                        "page_title": {
                            "type": "string",
                            "title": "Page Title",
                            "description": "The title to use for the chat interface.",
                            "default": "Large Language Model"
                        },
                        "llm_max_tokens": {
                            "type": "number",
                            "title": "Max Tokens",
                            "description": "The maximum number of new [tokens](https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_tokens) to generate for each LLM responses.",
                            "default": 1000
                        },
                        "llm_temperature": {
                            "type": "number",
                            "title": "LLM Temperature",
                            "description": "The [temperature](https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature) value to use when generating LLM responses.",
                            "default": 1
                        },
                        "llm_top_p": {
                            "type": "number",
                            "title": "LLM Top P",
                            "description": "The [top p](https://platform.openai.com/docs/api-reference/chat/create#chat-create-top_p) value to use when generating LLM responses.",
                            "default": 1
                        },
                        "llm_top_k": {
                            "type": "number",
                            "title": "LLM Top K",
                            "description": "The [top k](https://docs.vllm.ai/en/latest/dev/sampling_params.html) value to use when generating LLM responses.",
                            "default": -1
                        },
                        "llm_presence_penalty": {
                            "type": "number",
                            "title": "LLM Presence Penalty",
                            "description": "The [presence penalty](https://platform.openai.com/docs/api-reference/chat/create#chat-create-presence_penalty) to use when generating LLM responses.",
                            "default": 0
                        },
                        "llm_frequency_penalty": {
                            "type": "number",
                            "title": "LLM Frequency Penalty",
                            "description": "The [frequency_penalty](https://platform.openai.com/docs/api-reference/chat/create#chat-create-frequency_penalty) to use when generating LLM responses.",
                            "default": 0
                        }

                    },
                    "required": ["hf_model_name", "hf_model_instruction"]
                }
            }
        }
    }
}
