# Default values for azimuth-llm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

huggingface:
  # The name of the HuggingFace model to use
  model: tiiuae/falcon-7b-instruct
  # Other (partially tested) options:
  # (some of which may not fit on a single GPU and will take a long time to download)
  # - meta-llama/Llama-2-7b-chat-hf #Â Requires licence token
  # - tiiuae/falcon-40b # Weights ~160GB disk size
  # - bigscience/bloom # Weights were trending towards ~360GB disk size

  # For private/gated huggingface models (e.g. Meta's Llama models) 
  # you must provide your own huggingface token, for details see:
  # https://huggingface.co/docs/hub/security-tokens
  
  # To do this, either provide the name of an existing secret on the cluster,
  # which should be created before installing this chart by running 
  # `kubectl create secret generic huggingface-token --from-env-file <file-name>`
  # where <file-name> is a file with the following contents:
  # HUGGING_FACE_HUB_TOKEN=<token-value>
  secretName:
  # OR FOR TESTING PURPOSES ONLY, you can instead provide the secret directly
  # as a chart value here (if secretName is set about then it will take priority)
  token: ""

# Configuration for the backend model serving API
api:
  # Container image config
  image:
    repository: ghcr.io/stackhpc/azimuth-llm-api-base
    version: fae060a
  # Service config 
  service:
    name: text-generation-inference
    type: ClusterIP
    zenith:
      enabled: false
      skipAuth: false
      label: Inference API
      iconUrl:
      description: |
        The raw inference API endpoints for the deployed LLM. 
        Public API docs are available [here](https://huggingface.github.io/text-generation-inference/#/Text%20Generation%20Inference)
  # Config
  storage:
  # Number of gpus to requests for each api pod instance
  # NOTE: This must be in the range 1 <= value <= N, where
  #       'N' is the number of GPUs available in a single 
  #       worker node on the target Kubernetes cluster.
  gpus: 1
  # The update strategy to use for the deployment
  # See https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment
  # NOTE: Changing this has implications for the number of additional GPU worker nodes required
  #       to preform a rolling zero-downtime update
  updateStrategy:
    rollingUpdate:
      maxSurge: 0%
      maxUnavailable: 100%

# Configuration for the frontend web interface
ui:
  # Container image config
  image:
    repository: ghcr.io/stackhpc/azimuth-llm-ui-base
    version: 7b1d77f
  # Service config 
  service:
    name: web-app
    type: ClusterIP
    zenith:
      enabled: true
      skipAuth: false
      label: Web Interface
      iconUrl: https://raw.githubusercontent.com/gradio-app/gradio/5524e590577769b0444a5332b8d444aafb0c5c12/js/app/public/static/img/logo.svg
      description: |
        A web-based user inferface for interacting with the deployed LLM.
  # The update strategy to use for the deployment
  updateStrategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%