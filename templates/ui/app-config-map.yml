apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-web-app
  labels:
    {{- include "azimuth-llm.labels" . | nindent 4 }}
data:
  app.py: |
    import huggingface_hub
    from huggingface_hub import InferenceClient
    import gradio as gr
    from startup import wait_for_backend

    backend_url = "http://{{ .Values.api.service.name }}.{{ .Release.Namespace }}.svc"
    wait_for_backend(backend_url)

    client = InferenceClient(model=backend_url)

    def inference(message, history):
        
        if message == "":
            yield ""

        partial_message = ""
        try:
            for token in client.text_generation(message, max_new_tokens=500, stream=True):
                partial_message += token
                # Strip text marker from generated output
                partial_message = partial_message.replace('<|endoftext|>', '')
                yield partial_message
        except huggingface_hub.inference._text_generation.ValidationError as e:
            raise gr.Error("Context length exceeded. Please clear the chat window.")

    gr.ChatInterface(
        inference,
        chatbot=gr.Chatbot(
            height=500,
            show_copy_button=True,
        ),
        title="Azimuth LLM",
        description="This is the demo UI for the Azimuth LLM application.",
        textbox=gr.Textbox(placeholder="Ask me anything...", container=False, scale=7),
        retry_btn="Retry",
        undo_btn="Undo",
        clear_btn="Clear",
    ).queue().launch(server_name="0.0.0.0")
  startup.py: |
    import requests, time

    def wait_for_backend(url):
        ready = False
        while not ready:
            try:
                ready = (requests.get(f'{url}/health').status_code == 200)
                print('Waiting for backend API to start')
                time.sleep(5)
            except requests.exceptions.ConnectionError as e:
                pass
        return