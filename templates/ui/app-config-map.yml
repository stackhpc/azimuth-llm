apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-web-app
  labels:
    {{- include "azimuth-llm.labels" . | nindent 4 }}
data:
  app.py: |
    import requests, json
    import gradio as gr
    from startup import wait_for_backend

    # NOTE: This url should match the chart's api service name & namespace
    backend_url = "http://{{ .Values.api.service.name }}.{{ .Release.Namespace }}.svc"
    wait_for_backend(backend_url)

    def inference(message, history):
        
        headers = {"User-Agent": "vLLM Client"}
        pload = {
            "prompt": message,
            "stream": True,
            "max_tokens": 128,
        }
        response = requests.post(f'{backend_url}/generate',
                                headers=headers,
                                json=pload,
                                stream=True)

        for chunk in response.iter_lines(chunk_size=8192,
                                        decode_unicode=False,
                                        delimiter=b"\0"):
            if chunk:
                data = json.loads(chunk.decode("utf-8"))
                output = data["text"][0]
                yield output


    gr.ChatInterface(
        inference,
        chatbot=gr.Chatbot(
            height=500,
            show_copy_button=True,
            # layout='panel',
        ),
        textbox=gr.Textbox(placeholder="Ask me anything...", container=False, scale=7),
        title="Large Language Model",
        retry_btn="Retry",
        undo_btn="Undo",
        clear_btn="Clear",
    ).queue().launch(server_name="0.0.0.0")
  startup.py: |
    import requests, time

    def wait_for_backend(url):
        ready = False
        while not ready:
            try:
                ready = (requests.get(f'{url}/docs').status_code == 200)
                print('Waiting for backend API to start')
                time.sleep(5)
            except requests.exceptions.ConnectionError as e:
                pass
        return