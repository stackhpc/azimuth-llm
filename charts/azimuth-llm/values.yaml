# Default values for azimuth-llm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

huggingface:
  # The name of the HuggingFace model to use
  # Use a yaml anchor to avoid duplication elsewhere
  model: &model-name microsoft/Phi-3.5-mini-instruct
  # A Jinja formatted chat template to provide to the language model.
  # See https://huggingface.co/blog/chat-templates for background info.
  # If not provided, the default template specified in the HuggingFace
  # model repository's tokenizer_config.json file is used. As explained
  # in the above blog post, the HF template key in tokenizer_config.json
  # is relatively new and not all HF models include a template in their
  # repo files yet. This chart value provides a hook to manually apply the
  # correct chat template for such models.
  chatTemplate:
  # For private/gated huggingface models (e.g. Meta's Llama models)
  # you must provide your own huggingface token, for details see:
  # https://huggingface.co/docs/hub/security-tokens

  # To do this, either provide the name of an existing secret on the cluster,
  # which should be created before installing this chart by running
  # `kubectl create secret generic huggingface-token --from-env-file <file-name>`
  # where <file-name> is a file with the following contents:
  # HUGGING_FACE_HUB_TOKEN=<token-value>
  secretName:
  # OR FOR TESTING PURPOSES ONLY, you can instead provide the secret directly
  # as a chart value here (if secretName is set above then it will take priority)
  token:
# Configuration for the backend model serving API
api:
  enabled: true
  # Container image config
  image:
    # Defaults to vllm/vllm-openai when api.gpus > 0,
    # ghrc.io/stackhpc/vllm-xpu when api.gpus > 0 and intelXPUsEnabled is true,
    # or ghrc.io/stackhpc/vllm-cpu when api.gpus == 0
    repository:
    version: v0.8.5.post1
  monitoring:
    enabled: true
  # The number of replicas for the backend deployment
  replicas: 1
  # Service config
  service:
    type: ClusterIP
    zenith:
      enabled: false
      skipAuth: false
      label: OpenAI API
      iconUrl: https://raw.githubusercontent.com/vllm-project/vllm/v0.2.7/docs/source/assets/logos/vllm-logo-only-light.png
      description: |
        The OpenAI API for the deployed LLM.
  # Standard ingress resource config for backend API
  ingress:
    enabled: false
    className: ""
    annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    hosts:
      - host: chart-example.local
        paths:
          - path: /v1
            pathType: ImplementationSpecific
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
  # Config for huggingface model cache volume
  # This is mounted at /root/.cache/huggingface in the api deployment
  cacheVolume:
    hostPath:
      path: /tmp/llm/huggingface-cache
  # Number of gpus to requests for each api pod instance
  # NOTE: This must be less than the number of GPUs available in a single
  # worker node on the target Kubernetes cluster.
  # NOTE: According to the vLLM docs found here
  # https://docs.vllm.ai/en/latest/serving/distributed_serving.html
  # distributed / multi-GPU support should be available, though it
  # has not been tested against this app.
  gpus: 1
  # Whether pods should request Intel GPUs as opposed to the default Nvidia GPUs
  intelXPUsEnabled: false
  # The update strategy to use for the deployment
  # See https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment
  # NOTE: The following RollingUpdate strategy offers a zero-downtime update but requires additional GPU worker nodes.
  # updateStrategy:
  #   type: RollingUpdate
  #   rollingUpdate:
  #     maxSurge: 1
  #     maxUnavailable: 0
  updateStrategy:
    type: Recreate
  # The value of the vLLM backend's max_model_len argument (if the model's default is not suitable)
  # https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#command-line-arguments-for-the-server
  modelMaxContextLength:
  # Extra args to supply to the vLLM backend, see
  # https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#command-line-arguments-for-the-server
  extraArgs: []
  # Pod node selector labels
  nodeSelector: {}
  # Pod tolerations
  tolerations: []
  # Pod affinities
  affinity: {}
  # Pod disruption budget config
  pdb:
    enabled: false
    # Only one of these should be set
    # minAvailable:
    # maxUnavailable:
# Configuration for the frontend web interface
ui:
  # Toggles installation of the gradio web UI
  enabled: true
  # Container image config
  image:
    repository: ghcr.io/stackhpc/azimuth-llm-chat-ui
    # Defaults to chart's appVersion
    tag:
    imagePullPolicy:
  # The settings to be passed to the frontend web app.
  # Format depends on the chosen UI image above. For each of the UIs
  # included in the web-apps/ folder of this git repository there is a
  # defaults.yml file (e.g. web-apps/text-chat/defaults.yml) listing all
  # available configuration options.
  appSettings:
    model_name: *model-name
    # Use local system fonts by default to avoid GDPR issues
    # with Gradio's defaults fonts which require fetching from
    # the Google fonts API. To restore default Gradio theme
    # fonts, remove the font and font-mono keys.
    theme_params:
      font:
        - sans-serif
        - Arial
      font_mono:
        - sans-serif
        - Arial
  # Service config
  service:
    type: ClusterIP
    zenith:
      enabled: true
      skipAuth: false
      label: Chat Interface
      iconUrl: https://raw.githubusercontent.com/gradio-app/gradio/5524e590577769b0444a5332b8d444aafb0c5c12/js/app/public/static/img/logo.svg
      description: |
        A web-based user inferface for interacting with the deployed LLM.
  # Standard ingress resource config for UI web app
  ingress:
    enabled: false
    className: ""
    annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    hosts:
      - host: chart-example.local
        paths:
          - path: /
            pathType: ImplementationSpecific
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
  # The update strategy to use for the deployment
  updateStrategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
  # Pod node selector labels
  nodeSelector: {}
  # Pod tolerations
  tolerations: []
  # Pod affinities
  affinity: {}
  # Pod disruption budget config
  pdb:
    enabled: false
    # Only one of these should be set
    # minAvailable:
    # maxUnavailable:
