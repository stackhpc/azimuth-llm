{{- if .Values.api.enabled -}}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-api
  labels:
    {{- include "azimuth-llm.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.api.replicas }}
  selector:
    matchLabels:
      {{- include "azimuth-llm.api-selectorLabels" . | nindent 6 }}
  strategy:
    {{- .Values.api.updateStrategy | toYaml | nindent 4 }}
  template:
    metadata:
      labels:
        {{- include "azimuth-llm.api-selectorLabels" . | nindent 8 }}
    spec:
      containers:
      - name: {{ .Release.Name }}-api
        {{ $gpuChart := ternary "ghcr.io/stackhpc/vllm-xpu" "vllm/vllm-openai" .Values.api.intelXPUsEnabled -}}
        {{ $imageRepo := .Values.api.image.repository | default (ternary "ghcr.io/stackhpc/vllm-cpu" $gpuChart (eq (.Values.api.gpus | int) 0)) -}}
        image: {{ printf "%s:%s" $imageRepo .Values.api.image.version }}
        ports:
        - name: api
          containerPort: 8000
        volumeMounts:
        - name: data
          mountPath: /root/.cache/huggingface
        args:
          - --model
          - {{ .Values.huggingface.model }}
          {{- include "azimuth-llm.chatTemplate" . | nindent 10 -}}
          {{- if .Values.api.modelMaxContextLength -}}
          - --max-model-len
          - {{ .Values.api.modelMaxContextLength | quote }}
          {{- end -}}
          {{- if and (not (has "--tokenizer-model" .Values.api.extraArgs)) (hasPrefix "mistralai/" .Values.huggingface.model) -}}
          - --tokenizer-mode
          - mistral
          {{- end -}}
          {{- if .Values.api.extraArgs -}}
          {{- .Values.api.extraArgs | toYaml | nindent 10 }}
          {{- end -}}
        {{- if .Values.huggingface.secretName -}}
        envFrom:
        - secretRef:
            name: {{ .Values.huggingface.secretName }}
        {{- end }}
        env:
        - name: DO_NOT_TRACK
          value: "1"
        {{- if .Values.huggingface.token }}
        - name: HUGGING_FACE_HUB_TOKEN
          value: {{ quote .Values.huggingface.token }}
        {{- end }}
        readinessProbe:
          httpGet:
            port: 8000
            path: /health
          periodSeconds: 10
        resources:
          limits:
            {{- if .Values.api.intelXPUsEnabled }}
            gpu.intel.com/i915: {{ .Values.api.gpus | int }}
            {{- else }}
            nvidia.com/gpu: {{ .Values.api.gpus | int }}
            {{- end }}
      volumes:
        - name: data
          {{- .Values.api.cacheVolume | toYaml | nindent 10 }}
        # Suggested in vLLM docs
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
      {{- with $.Values.api.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with $.Values.api.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with $.Values.api.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
{{- end -}}
